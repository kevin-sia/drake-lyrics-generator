{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow text generation with an RNN tutorial](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow GPU memory growth must be limited to allow model to train (was having issues without doing this).  Code in below cell borrowed from the [TensorFlow documentation](https://www.tensorflow.org/guide/gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# limiting GPU memory growth\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # enabling memory growth\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPU')\n",
    "    except RuntimeError as e:\n",
    "        # memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text must all be in a single `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "text = str(open('data/drake_lyrics.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo what's goin on, this is Drake\n",
      "And I'ma let you know what you about to witness aight?\n",
      "This right here, is a Drake, and DJ Smallz collaboration\n",
      "So I'm from Canada, my mans from down South\n",
      "You understand the #1 DJ in the South to be exact\n",
      "You heard t\n"
     ]
    }
   ],
   "source": [
    "# peek into file\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().replace('\\n', ' \\n ')\n",
    "\n",
    "for punc in punctuation:\n",
    "    words = words.replace(punc, '')\n",
    "\n",
    "words = words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(file='vocab', mode='wb')\n",
    "pickle.dump(vocab, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\n',\n",
       " '0',\n",
       " '000',\n",
       " '000s',\n",
       " '010',\n",
       " '02',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '09',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1017',\n",
       " '11',\n",
       " '110',\n",
       " '11th',\n",
       " '12',\n",
       " '12bedroom',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '1503',\n",
       " '15th',\n",
       " '16',\n",
       " '16s',\n",
       " '17',\n",
       " '1799',\n",
       " '18',\n",
       " '19',\n",
       " '1991',\n",
       " '1998',\n",
       " '1da',\n",
       " '1s',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2007',\n",
       " '2008',\n",
       " '2010',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '20s',\n",
       " '21',\n",
       " '22',\n",
       " '224',\n",
       " '23',\n",
       " '23s',\n",
       " '24',\n",
       " '247',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '281',\n",
       " '29',\n",
       " '2999',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '305',\n",
       " '31st',\n",
       " '325',\n",
       " '33rd',\n",
       " '35',\n",
       " '360',\n",
       " '3months',\n",
       " '3rd',\n",
       " '3s',\n",
       " '4',\n",
       " '40',\n",
       " '4000',\n",
       " '401',\n",
       " '4040',\n",
       " '4201',\n",
       " '4301',\n",
       " '44',\n",
       " '48',\n",
       " '4th',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '504',\n",
       " '50k',\n",
       " '50ms',\n",
       " '52',\n",
       " '54',\n",
       " '5s',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60000',\n",
       " '61',\n",
       " '62',\n",
       " '6449393',\n",
       " '645',\n",
       " '66',\n",
       " '680',\n",
       " '6am',\n",
       " '6er',\n",
       " '6ix',\n",
       " '6s',\n",
       " '7',\n",
       " '70',\n",
       " '70s',\n",
       " '747',\n",
       " '7am',\n",
       " '8',\n",
       " '80',\n",
       " '808',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '876',\n",
       " '8am',\n",
       " '8th',\n",
       " '9',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '937',\n",
       " '94',\n",
       " '96',\n",
       " '97',\n",
       " '99',\n",
       " '9am',\n",
       " '9th',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aaaallll',\n",
       " 'aahhck',\n",
       " 'aaliyah',\n",
       " 'aaliyahs',\n",
       " 'aaron',\n",
       " 'abc',\n",
       " 'abcs',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abridge',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'abu',\n",
       " 'abusing',\n",
       " 'ac',\n",
       " 'academy',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'acceptin',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accolades',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounts',\n",
       " 'accustom',\n",
       " 'ace',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'acknowledgement',\n",
       " 'acreaming',\n",
       " 'acres',\n",
       " 'acrobat',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actavis',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activis',\n",
       " 'actors',\n",
       " 'actually',\n",
       " 'acura',\n",
       " 'adams',\n",
       " 'adanunana',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'adddriss',\n",
       " 'adderall',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addins',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressin',\n",
       " 'addressing',\n",
       " 'addy',\n",
       " 'adel',\n",
       " 'adele',\n",
       " 'adjust',\n",
       " 'adjusting',\n",
       " 'admire',\n",
       " 'admirers',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'adunanane',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'advice',\n",
       " 'advices',\n",
       " 'advil',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'af',\n",
       " 'affairs',\n",
       " 'affection',\n",
       " 'affiliation',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'afriad',\n",
       " 'africa',\n",
       " 'afrocentric',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterparty',\n",
       " 'afura',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aghh',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'ah',\n",
       " 'ahahaha',\n",
       " 'ahahahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhh',\n",
       " 'ahhsookiesookie',\n",
       " 'ahhw',\n",
       " 'ahhww',\n",
       " 'ahout',\n",
       " 'ahoy',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aight',\n",
       " 'aiight',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'ain’t',\n",
       " 'air',\n",
       " 'airin',\n",
       " 'airing',\n",
       " 'airlines',\n",
       " 'airplay',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'airwaves',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akeem',\n",
       " 'akinyele',\n",
       " 'akron',\n",
       " 'al',\n",
       " 'alamo',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'aleady',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'aleshia',\n",
       " 'alexander',\n",
       " 'alfredo',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alicia’s',\n",
       " 'alien',\n",
       " 'alienated',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alisha',\n",
       " 'alist',\n",
       " 'alive',\n",
       " 'alizein',\n",
       " 'all',\n",
       " 'allamerican',\n",
       " 'allblack',\n",
       " 'allegations',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'alley',\n",
       " 'alleyoop',\n",
       " 'alllllright',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allows',\n",
       " 'allstar',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'aloha’s',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alota',\n",
       " 'aloud',\n",
       " 'alpo',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'alters',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'always',\n",
       " 'alyx',\n",
       " 'al–',\n",
       " 'am',\n",
       " 'ama',\n",
       " 'amazed',\n",
       " 'amazin',\n",
       " 'amazing',\n",
       " 'amber',\n",
       " 'ambiance',\n",
       " 'ambidex',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'amen',\n",
       " 'amend',\n",
       " 'amendment',\n",
       " 'amends',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americas',\n",
       " 'amex',\n",
       " 'amg',\n",
       " 'amiri',\n",
       " 'ammo',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amstel',\n",
       " 'amsterdam',\n",
       " 'amusin',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'anatomy',\n",
       " 'and',\n",
       " 'andale',\n",
       " 'andandandand',\n",
       " 'andandandand—',\n",
       " 'anderson',\n",
       " 'andos',\n",
       " 'andreena',\n",
       " 'andy',\n",
       " 'aneisha',\n",
       " 'anemic',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelou',\n",
       " 'angels',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'anita',\n",
       " 'ankle',\n",
       " 'anna',\n",
       " 'annual',\n",
       " 'anointed',\n",
       " 'anomaly',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'ante',\n",
       " 'anthems',\n",
       " 'anthony',\n",
       " 'anthrax',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'antisocial',\n",
       " 'antlers',\n",
       " 'antonio',\n",
       " 'anxieties',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anythings',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'an’',\n",
       " 'aoh',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'apocalypse',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'appeal',\n",
       " 'appealin',\n",
       " 'appealing',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applebees',\n",
       " 'appleton',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appreciate',\n",
       " 'apprehend',\n",
       " 'apprehended',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approachable',\n",
       " 'approached',\n",
       " 'approve',\n",
       " 'apps',\n",
       " 'aquafina',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arch',\n",
       " 'architects',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'arenas',\n",
       " 'arent',\n",
       " 'argue',\n",
       " 'arguin',\n",
       " 'arguing',\n",
       " 'aria',\n",
       " 'arm',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'aromas',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'ars',\n",
       " 'arsehole',\n",
       " 'art',\n",
       " 'artful',\n",
       " 'arthur',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'as',\n",
       " 'ash',\n",
       " 'ashame',\n",
       " 'ashamed',\n",
       " 'ashely',\n",
       " 'ashley',\n",
       " 'ashore',\n",
       " 'asian',\n",
       " 'asics',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspahrin',\n",
       " 'aspirations',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assembled',\n",
       " 'assembly',\n",
       " 'asses',\n",
       " 'asset',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assistants',\n",
       " 'assistin',\n",
       " 'assists',\n",
       " 'assume',\n",
       " 'assumin',\n",
       " 'assuming',\n",
       " 'assumptions',\n",
       " 'assure',\n",
       " 'asthma',\n",
       " 'asthmatic',\n",
       " 'astonishing',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atf',\n",
       " 'athletes',\n",
       " 'atlanta',\n",
       " 'atlas',\n",
       " 'atown',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacking',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attendant',\n",
       " 'attending',\n",
       " 'attenion',\n",
       " 'attention',\n",
       " 'attic',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'aubrey',\n",
       " 'aubster',\n",
       " 'auction',\n",
       " 'audacity',\n",
       " 'audemar',\n",
       " 'audemars',\n",
       " 'audi',\n",
       " 'audience',\n",
       " 'audit',\n",
       " 'audition',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunty',\n",
       " 'aura',\n",
       " 'austin',\n",
       " 'authentic',\n",
       " 'authority',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'automatic',\n",
       " 'automin',\n",
       " 'autopilot',\n",
       " 'autumn',\n",
       " 'availability',\n",
       " 'ave',\n",
       " 'aventura',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avis',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awlins',\n",
       " 'aww',\n",
       " 'awwhh',\n",
       " 'awwmpits',\n",
       " 'awww',\n",
       " 'axl',\n",
       " 'ay',\n",
       " 'ayatollah',\n",
       " 'aye',\n",
       " 'ayo',\n",
       " 'ayy',\n",
       " 'ayye',\n",
       " 'a—',\n",
       " 'b',\n",
       " 'baaaybeeeee',\n",
       " 'babayyyy',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babyface',\n",
       " 'babygirl',\n",
       " 'babyheat',\n",
       " 'babylon',\n",
       " 'babys',\n",
       " 'babysit',\n",
       " 'baccarat',\n",
       " 'bachelor',\n",
       " 'back',\n",
       " 'backflips',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backing',\n",
       " 'backpack',\n",
       " 'backpackers',\n",
       " 'backpacks',\n",
       " 'backpedal',\n",
       " 'backs',\n",
       " 'backseat',\n",
       " 'backstabbed',\n",
       " 'backstage',\n",
       " 'backtrack',\n",
       " 'backwards',\n",
       " 'backwoods',\n",
       " 'backyard',\n",
       " 'back”',\n",
       " 'bad',\n",
       " 'badder',\n",
       " 'baddest',\n",
       " 'badge',\n",
       " 'badou',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'bagel',\n",
       " 'baggage',\n",
       " 'bagging',\n",
       " 'bags',\n",
       " 'bail',\n",
       " 'bajan',\n",
       " 'baka',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'bakin',\n",
       " 'balance',\n",
       " 'balances',\n",
       " 'balcony',\n",
       " 'baldi',\n",
       " 'bale',\n",
       " 'bales',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'ballet',\n",
       " 'ballin',\n",
       " 'balling',\n",
       " 'ballistic',\n",
       " 'ballpoint',\n",
       " 'balls',\n",
       " 'ballsy',\n",
       " 'balotelli',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandaid',\n",
       " 'bandana',\n",
       " 'bandanas',\n",
       " 'bandanna',\n",
       " 'bandit',\n",
       " 'bands',\n",
       " 'bandwagon',\n",
       " 'bandwagons',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bangers',\n",
       " 'bangin',\n",
       " 'banging',\n",
       " 'banished',\n",
       " 'banister',\n",
       " 'bank',\n",
       " 'bankhead',\n",
       " 'bankin',\n",
       " 'banking',\n",
       " 'bankroll',\n",
       " 'banks',\n",
       " 'banner',\n",
       " 'banners',\n",
       " 'baow',\n",
       " 'baptized',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barbados',\n",
       " 'barber',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'bargained',\n",
       " 'baritone',\n",
       " 'bark',\n",
       " 'barker',\n",
       " 'barket',\n",
       " 'barley',\n",
       " 'barmitsfa',\n",
       " 'barney',\n",
       " 'barneys',\n",
       " 'barney’s',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'bars',\n",
       " 'bartending',\n",
       " 'baruch',\n",
       " 'barz',\n",
       " 'barzini',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'bases',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastid',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bathe',\n",
       " 'bathroom',\n",
       " 'batman',\n",
       " 'batphone',\n",
       " 'battered',\n",
       " 'batteries',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'battleship',\n",
       " 'battlin',\n",
       " 'baxter',\n",
       " 'bay',\n",
       " 'bayside',\n",
       " 'bbbbet',\n",
       " 'bbk',\n",
       " 'bboys',\n",
       " 'bbring',\n",
       " 'bcbg',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beaches',\n",
       " 'beachfront',\n",
       " 'bead',\n",
       " 'beads',\n",
       " 'beakin',\n",
       " 'beaky',\n",
       " 'beam',\n",
       " 'beamer',\n",
       " 'beamin',\n",
       " 'bean',\n",
       " 'beansa',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaters',\n",
       " 'beatin',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'beaver',\n",
       " 'beavers',\n",
       " 'bebe',\n",
       " 'became',\n",
       " 'because',\n",
       " 'beckham',\n",
       " 'becky',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'bedrooms',\n",
       " 'bedtime',\n",
       " 'beef',\n",
       " 'beefin',\n",
       " 'been',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'befo',\n",
       " 'before',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begets',\n",
       " 'beggin',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'beginners',\n",
       " 'beginnin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behav',\n",
       " 'behave',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'beige',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'belair',\n",
       " 'belated',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believer',\n",
       " 'bell',\n",
       " 'bells',\n",
       " 'belong',\n",
       " 'belongings',\n",
       " 'belongs',\n",
       " 'below',\n",
       " 'belt',\n",
       " 'belted',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'bended',\n",
       " 'bendel',\n",
       " 'bender',\n",
       " 'bendin',\n",
       " 'benedict',\n",
       " 'benefit',\n",
       " 'benihana',\n",
       " 'benny',\n",
       " 'bent',\n",
       " 'bentley',\n",
       " 'bentleys',\n",
       " 'benz',\n",
       " 'benzes',\n",
       " 'benzfull',\n",
       " 'benzo',\n",
       " 'beretta',\n",
       " 'berkstown',\n",
       " 'bernie',\n",
       " 'berry',\n",
       " 'bes',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'betrayed',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beverage',\n",
       " 'beverly',\n",
       " 'beyoncé',\n",
       " 'beyond',\n",
       " 'bezel',\n",
       " 'bianca',\n",
       " 'biased',\n",
       " 'bib',\n",
       " 'bible',\n",
       " 'biblical',\n",
       " 'bid',\n",
       " 'biddin',\n",
       " 'biem',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bigging',\n",
       " 'biggs',\n",
       " 'biggy',\n",
       " 'bigtymer',\n",
       " 'bih',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billboard',\n",
       " 'billboards',\n",
       " 'billie',\n",
       " 'billies',\n",
       " 'billin',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'billis',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'bingewatchin',\n",
       " 'bird',\n",
       " 'birdman',\n",
       " 'birds',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'birthed',\n",
       " 'birthplace',\n",
       " 'bis',\n",
       " 'biscayne',\n",
       " 'bishop',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'bitin',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'bizzench',\n",
       " 'bjarke',\n",
       " 'blabbin',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blackdiamond',\n",
       " 'blacker',\n",
       " 'blackglasses',\n",
       " 'blackin',\n",
       " 'blacking',\n",
       " 'blade',\n",
       " 'blades',\n",
       " 'blaine',\n",
       " 'blaka',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blanca',\n",
       " 'blanco',\n",
       " 'blang',\n",
       " 'blank',\n",
       " 'blankets',\n",
       " 'blaqnmild',\n",
       " 'blast',\n",
       " 'blastin',\n",
       " 'blasting',\n",
       " 'blat',\n",
       " 'blatant',\n",
       " 'blaze',\n",
       " 'bleachers',\n",
       " 'bleed',\n",
       " 'blem',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessin',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'bleu',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'bling',\n",
       " 'blingin',\n",
       " 'blink',\n",
       " 'blinkers',\n",
       " 'blistering',\n",
       " 'blitz',\n",
       " 'blk',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'blockswitchin',\n",
       " 'blogging',\n",
       " 'blogs',\n",
       " 'blood',\n",
       " 'bloods',\n",
       " 'bloodshot',\n",
       " 'blouse',\n",
       " 'blow',\n",
       " 'blowin',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blows',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'bluff',\n",
       " 'bluffin',\n",
       " 'bluffing',\n",
       " 'blunt',\n",
       " 'blunted',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open(file='vocab', mode='rb')\n",
    "vocab = pickle.load(infile)\n",
    "infile.close()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9597"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['yo', 'whats', 'goin', 'on', 'this', 'is', 'drake', '\\n', 'and', 'ima', 'let', 'you', 'know']\" ---- characters mapped to int ---- > [9515 9247 3558 5767 8480 4294 2565    1  379 4161 4767 9522 4613]\n"
     ]
    }
   ],
   "source": [
    "# map unique characters to indices\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# reverse the map - use this to specify an index to obtain a character\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "# entire text document represented in the above character-to-indices mapping\n",
    "words_as_int = np.array([word2idx[c] for c in words])\n",
    "\n",
    "# sample\n",
    "print(f'\"{words[:13]}\" ---- characters mapped to int ---- > {words_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle this since it is needed in text generation\n",
    "outfile = open(file='pkl/word2idx', mode='wb')\n",
    "pickle.dump(word2idx, outfile)\n",
    "outfile.close()\n",
    "\n",
    "# pickle this since it is needed in text generation\n",
    "outfile = open(file='pkl/idx2word', mode='wb')\n",
    "pickle.dump(idx2word, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(file='pkl/word2idx', mode='rb')\n",
    "word2idx = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\n',\n",
       " '0',\n",
       " '000',\n",
       " '000s',\n",
       " '010',\n",
       " '02',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '09',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1017',\n",
       " '11',\n",
       " '110',\n",
       " '11th',\n",
       " '12',\n",
       " '12bedroom',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '1503',\n",
       " '15th',\n",
       " '16',\n",
       " '16s',\n",
       " '17',\n",
       " '1799',\n",
       " '18',\n",
       " '19',\n",
       " '1991',\n",
       " '1998',\n",
       " '1da',\n",
       " '1s',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2007',\n",
       " '2008',\n",
       " '2010',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '20s',\n",
       " '21',\n",
       " '22',\n",
       " '224',\n",
       " '23',\n",
       " '23s',\n",
       " '24',\n",
       " '247',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '281',\n",
       " '29',\n",
       " '2999',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '305',\n",
       " '31st',\n",
       " '325',\n",
       " '33rd',\n",
       " '35',\n",
       " '360',\n",
       " '3months',\n",
       " '3rd',\n",
       " '3s',\n",
       " '4',\n",
       " '40',\n",
       " '4000',\n",
       " '401',\n",
       " '4040',\n",
       " '4201',\n",
       " '4301',\n",
       " '44',\n",
       " '48',\n",
       " '4th',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '504',\n",
       " '50k',\n",
       " '50ms',\n",
       " '52',\n",
       " '54',\n",
       " '5s',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60000',\n",
       " '61',\n",
       " '62',\n",
       " '6449393',\n",
       " '645',\n",
       " '66',\n",
       " '680',\n",
       " '6am',\n",
       " '6er',\n",
       " '6ix',\n",
       " '6s',\n",
       " '7',\n",
       " '70',\n",
       " '70s',\n",
       " '747',\n",
       " '7am',\n",
       " '8',\n",
       " '80',\n",
       " '808',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '876',\n",
       " '8am',\n",
       " '8th',\n",
       " '9',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '937',\n",
       " '94',\n",
       " '96',\n",
       " '97',\n",
       " '99',\n",
       " '9am',\n",
       " '9th',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aaaallll',\n",
       " 'aahhck',\n",
       " 'aaliyah',\n",
       " 'aaliyahs',\n",
       " 'aaron',\n",
       " 'abc',\n",
       " 'abcs',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abridge',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'abu',\n",
       " 'abusing',\n",
       " 'ac',\n",
       " 'academy',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'acceptin',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accolades',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounts',\n",
       " 'accustom',\n",
       " 'ace',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'acknowledgement',\n",
       " 'acreaming',\n",
       " 'acres',\n",
       " 'acrobat',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actavis',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activis',\n",
       " 'actors',\n",
       " 'actually',\n",
       " 'acura',\n",
       " 'adams',\n",
       " 'adanunana',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'adddriss',\n",
       " 'adderall',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addins',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressin',\n",
       " 'addressing',\n",
       " 'addy',\n",
       " 'adel',\n",
       " 'adele',\n",
       " 'adjust',\n",
       " 'adjusting',\n",
       " 'admire',\n",
       " 'admirers',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'adunanane',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'advice',\n",
       " 'advices',\n",
       " 'advil',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'af',\n",
       " 'affairs',\n",
       " 'affection',\n",
       " 'affiliation',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'afriad',\n",
       " 'africa',\n",
       " 'afrocentric',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterparty',\n",
       " 'afura',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aghh',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'ah',\n",
       " 'ahahaha',\n",
       " 'ahahahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhh',\n",
       " 'ahhsookiesookie',\n",
       " 'ahhw',\n",
       " 'ahhww',\n",
       " 'ahout',\n",
       " 'ahoy',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aight',\n",
       " 'aiight',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'ain’t',\n",
       " 'air',\n",
       " 'airin',\n",
       " 'airing',\n",
       " 'airlines',\n",
       " 'airplay',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'airwaves',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akeem',\n",
       " 'akinyele',\n",
       " 'akron',\n",
       " 'al',\n",
       " 'alamo',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'aleady',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'aleshia',\n",
       " 'alexander',\n",
       " 'alfredo',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alicia’s',\n",
       " 'alien',\n",
       " 'alienated',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alisha',\n",
       " 'alist',\n",
       " 'alive',\n",
       " 'alizein',\n",
       " 'all',\n",
       " 'allamerican',\n",
       " 'allblack',\n",
       " 'allegations',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'alley',\n",
       " 'alleyoop',\n",
       " 'alllllright',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allows',\n",
       " 'allstar',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'aloha’s',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alota',\n",
       " 'aloud',\n",
       " 'alpo',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'alters',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'always',\n",
       " 'alyx',\n",
       " 'al–',\n",
       " 'am',\n",
       " 'ama',\n",
       " 'amazed',\n",
       " 'amazin',\n",
       " 'amazing',\n",
       " 'amber',\n",
       " 'ambiance',\n",
       " 'ambidex',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'amen',\n",
       " 'amend',\n",
       " 'amendment',\n",
       " 'amends',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americas',\n",
       " 'amex',\n",
       " 'amg',\n",
       " 'amiri',\n",
       " 'ammo',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amstel',\n",
       " 'amsterdam',\n",
       " 'amusin',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'anatomy',\n",
       " 'and',\n",
       " 'andale',\n",
       " 'andandandand',\n",
       " 'andandandand—',\n",
       " 'anderson',\n",
       " 'andos',\n",
       " 'andreena',\n",
       " 'andy',\n",
       " 'aneisha',\n",
       " 'anemic',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelou',\n",
       " 'angels',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'anita',\n",
       " 'ankle',\n",
       " 'anna',\n",
       " 'annual',\n",
       " 'anointed',\n",
       " 'anomaly',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'ante',\n",
       " 'anthems',\n",
       " 'anthony',\n",
       " 'anthrax',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'antisocial',\n",
       " 'antlers',\n",
       " 'antonio',\n",
       " 'anxieties',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anythings',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'an’',\n",
       " 'aoh',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'apocalypse',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'appeal',\n",
       " 'appealin',\n",
       " 'appealing',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applebees',\n",
       " 'appleton',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appreciate',\n",
       " 'apprehend',\n",
       " 'apprehended',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approachable',\n",
       " 'approached',\n",
       " 'approve',\n",
       " 'apps',\n",
       " 'aquafina',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arch',\n",
       " 'architects',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'arenas',\n",
       " 'arent',\n",
       " 'argue',\n",
       " 'arguin',\n",
       " 'arguing',\n",
       " 'aria',\n",
       " 'arm',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'aromas',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'ars',\n",
       " 'arsehole',\n",
       " 'art',\n",
       " 'artful',\n",
       " 'arthur',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'as',\n",
       " 'ash',\n",
       " 'ashame',\n",
       " 'ashamed',\n",
       " 'ashely',\n",
       " 'ashley',\n",
       " 'ashore',\n",
       " 'asian',\n",
       " 'asics',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspahrin',\n",
       " 'aspirations',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assembled',\n",
       " 'assembly',\n",
       " 'asses',\n",
       " 'asset',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assistants',\n",
       " 'assistin',\n",
       " 'assists',\n",
       " 'assume',\n",
       " 'assumin',\n",
       " 'assuming',\n",
       " 'assumptions',\n",
       " 'assure',\n",
       " 'asthma',\n",
       " 'asthmatic',\n",
       " 'astonishing',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atf',\n",
       " 'athletes',\n",
       " 'atlanta',\n",
       " 'atlas',\n",
       " 'atown',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacking',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attendant',\n",
       " 'attending',\n",
       " 'attenion',\n",
       " 'attention',\n",
       " 'attic',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'aubrey',\n",
       " 'aubster',\n",
       " 'auction',\n",
       " 'audacity',\n",
       " 'audemar',\n",
       " 'audemars',\n",
       " 'audi',\n",
       " 'audience',\n",
       " 'audit',\n",
       " 'audition',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunty',\n",
       " 'aura',\n",
       " 'austin',\n",
       " 'authentic',\n",
       " 'authority',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'automatic',\n",
       " 'automin',\n",
       " 'autopilot',\n",
       " 'autumn',\n",
       " 'availability',\n",
       " 'ave',\n",
       " 'aventura',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avis',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awlins',\n",
       " 'aww',\n",
       " 'awwhh',\n",
       " 'awwmpits',\n",
       " 'awww',\n",
       " 'axl',\n",
       " 'ay',\n",
       " 'ayatollah',\n",
       " 'aye',\n",
       " 'ayo',\n",
       " 'ayy',\n",
       " 'ayye',\n",
       " 'a—',\n",
       " 'b',\n",
       " 'baaaybeeeee',\n",
       " 'babayyyy',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babyface',\n",
       " 'babygirl',\n",
       " 'babyheat',\n",
       " 'babylon',\n",
       " 'babys',\n",
       " 'babysit',\n",
       " 'baccarat',\n",
       " 'bachelor',\n",
       " 'back',\n",
       " 'backflips',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backing',\n",
       " 'backpack',\n",
       " 'backpackers',\n",
       " 'backpacks',\n",
       " 'backpedal',\n",
       " 'backs',\n",
       " 'backseat',\n",
       " 'backstabbed',\n",
       " 'backstage',\n",
       " 'backtrack',\n",
       " 'backwards',\n",
       " 'backwoods',\n",
       " 'backyard',\n",
       " 'back”',\n",
       " 'bad',\n",
       " 'badder',\n",
       " 'baddest',\n",
       " 'badge',\n",
       " 'badou',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'bagel',\n",
       " 'baggage',\n",
       " 'bagging',\n",
       " 'bags',\n",
       " 'bail',\n",
       " 'bajan',\n",
       " 'baka',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'bakin',\n",
       " 'balance',\n",
       " 'balances',\n",
       " 'balcony',\n",
       " 'baldi',\n",
       " 'bale',\n",
       " 'bales',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'ballet',\n",
       " 'ballin',\n",
       " 'balling',\n",
       " 'ballistic',\n",
       " 'ballpoint',\n",
       " 'balls',\n",
       " 'ballsy',\n",
       " 'balotelli',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandaid',\n",
       " 'bandana',\n",
       " 'bandanas',\n",
       " 'bandanna',\n",
       " 'bandit',\n",
       " 'bands',\n",
       " 'bandwagon',\n",
       " 'bandwagons',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bangers',\n",
       " 'bangin',\n",
       " 'banging',\n",
       " 'banished',\n",
       " 'banister',\n",
       " 'bank',\n",
       " 'bankhead',\n",
       " 'bankin',\n",
       " 'banking',\n",
       " 'bankroll',\n",
       " 'banks',\n",
       " 'banner',\n",
       " 'banners',\n",
       " 'baow',\n",
       " 'baptized',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barbados',\n",
       " 'barber',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'bargained',\n",
       " 'baritone',\n",
       " 'bark',\n",
       " 'barker',\n",
       " 'barket',\n",
       " 'barley',\n",
       " 'barmitsfa',\n",
       " 'barney',\n",
       " 'barneys',\n",
       " 'barney’s',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'bars',\n",
       " 'bartending',\n",
       " 'baruch',\n",
       " 'barz',\n",
       " 'barzini',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'bases',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastid',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bathe',\n",
       " 'bathroom',\n",
       " 'batman',\n",
       " 'batphone',\n",
       " 'battered',\n",
       " 'batteries',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'battleship',\n",
       " 'battlin',\n",
       " 'baxter',\n",
       " 'bay',\n",
       " 'bayside',\n",
       " 'bbbbet',\n",
       " 'bbk',\n",
       " 'bboys',\n",
       " 'bbring',\n",
       " 'bcbg',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beaches',\n",
       " 'beachfront',\n",
       " 'bead',\n",
       " 'beads',\n",
       " 'beakin',\n",
       " 'beaky',\n",
       " 'beam',\n",
       " 'beamer',\n",
       " 'beamin',\n",
       " 'bean',\n",
       " 'beansa',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaters',\n",
       " 'beatin',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'beaver',\n",
       " 'beavers',\n",
       " 'bebe',\n",
       " 'became',\n",
       " 'because',\n",
       " 'beckham',\n",
       " 'becky',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'bedrooms',\n",
       " 'bedtime',\n",
       " 'beef',\n",
       " 'beefin',\n",
       " 'been',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'befo',\n",
       " 'before',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begets',\n",
       " 'beggin',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'beginners',\n",
       " 'beginnin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behav',\n",
       " 'behave',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'beige',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'belair',\n",
       " 'belated',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believer',\n",
       " 'bell',\n",
       " 'bells',\n",
       " 'belong',\n",
       " 'belongings',\n",
       " 'belongs',\n",
       " 'below',\n",
       " 'belt',\n",
       " 'belted',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'bended',\n",
       " 'bendel',\n",
       " 'bender',\n",
       " 'bendin',\n",
       " 'benedict',\n",
       " 'benefit',\n",
       " 'benihana',\n",
       " 'benny',\n",
       " 'bent',\n",
       " 'bentley',\n",
       " 'bentleys',\n",
       " 'benz',\n",
       " 'benzes',\n",
       " 'benzfull',\n",
       " 'benzo',\n",
       " 'beretta',\n",
       " 'berkstown',\n",
       " 'bernie',\n",
       " 'berry',\n",
       " 'bes',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'betrayed',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beverage',\n",
       " 'beverly',\n",
       " 'beyoncé',\n",
       " 'beyond',\n",
       " 'bezel',\n",
       " 'bianca',\n",
       " 'biased',\n",
       " 'bib',\n",
       " 'bible',\n",
       " 'biblical',\n",
       " 'bid',\n",
       " 'biddin',\n",
       " 'biem',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bigging',\n",
       " 'biggs',\n",
       " 'biggy',\n",
       " 'bigtymer',\n",
       " 'bih',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billboard',\n",
       " 'billboards',\n",
       " 'billie',\n",
       " 'billies',\n",
       " 'billin',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'billis',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'bingewatchin',\n",
       " 'bird',\n",
       " 'birdman',\n",
       " 'birds',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'birthed',\n",
       " 'birthplace',\n",
       " 'bis',\n",
       " 'biscayne',\n",
       " 'bishop',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'bitin',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'bizzench',\n",
       " 'bjarke',\n",
       " 'blabbin',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blackdiamond',\n",
       " 'blacker',\n",
       " 'blackglasses',\n",
       " 'blackin',\n",
       " 'blacking',\n",
       " 'blade',\n",
       " 'blades',\n",
       " 'blaine',\n",
       " 'blaka',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blanca',\n",
       " 'blanco',\n",
       " 'blang',\n",
       " 'blank',\n",
       " 'blankets',\n",
       " 'blaqnmild',\n",
       " 'blast',\n",
       " 'blastin',\n",
       " 'blasting',\n",
       " 'blat',\n",
       " 'blatant',\n",
       " 'blaze',\n",
       " 'bleachers',\n",
       " 'bleed',\n",
       " 'blem',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessin',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'bleu',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'bling',\n",
       " 'blingin',\n",
       " 'blink',\n",
       " 'blinkers',\n",
       " 'blistering',\n",
       " 'blitz',\n",
       " 'blk',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'blockswitchin',\n",
       " 'blogging',\n",
       " 'blogs',\n",
       " 'blood',\n",
       " 'bloods',\n",
       " 'bloodshot',\n",
       " 'blouse',\n",
       " 'blow',\n",
       " 'blowin',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blows',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'bluff',\n",
       " 'bluffin',\n",
       " 'bluffing',\n",
       " 'blunt',\n",
       " 'blunted',\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is **character vectorization**.  Word vectorization would probably make more coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # map unique characters to indices\n",
    "# char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# # reverse the map - use this to specify an index to obtain a character\n",
    "# idx2char = np.array(vocab)\n",
    "\n",
    "# # entire text document represented in the above character-to-indices mapping\n",
    "# text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# # sample\n",
    "# print(f'\"{text[:13]}\" ---- characters mapped to int ---- > {text_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Examples & Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model input**: sequence of characters\n",
    "\n",
    "**model output (prediction)**: the following character at each step (based on previous characters in the sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the text into **example sequences**.  Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "**For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.**\n",
    "\n",
    "So, break the text into chunks of `seq_length + 1`.  e.g. if `seq_length` is 4 and our text is \"Hello\", the input sequence would be \"Hell\" and the target sequence would be \"ello\".\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices` converts the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "# max sentence length (in number of characters) desired for single input\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(words) // (seq_length + 1) # floored division\n",
    "\n",
    "# create training examples/targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(words_as_int)\n",
    "\n",
    "# data type of train examples/targets\n",
    "print(type(word_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "whats\n",
      "goin\n",
      "on\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "# preview training examples as characters (using the indices in word_dataset)\n",
    "for i in word_dataset.take(5):\n",
    "    print(idx2word[i.numpy()]) # .numpy() converts into numpy data format (in this case, a numpy integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `batch` method on `char_dataset` (type `tensorflow.python.data.ops.dataset_ops.TensorSliceDataset`) to convert the individual characters to sequences of the desired size (`seq_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> \n",
      "\n",
      "'yo whats goin on this is drake \\n and ima let you know what you about to witness aight \\n this right here is a drake and dj smallz collaboration \\n so im from canada my mans from down south \\n you understand the 1 dj in the south to be exact \\n you heard that at the vmas you heard it wherever he goes \\n my man smallz is out there down south \\n same time reppin for toronto canada yknahmean \\n so this right here what you bout what you listenin to right now \\n is the official southern'\n",
      "'smoke special edition \\n i call it the room for improvement mixtape \\n cause im not perfect and i bet neither are you if you listenin \\n so you need to just accept whats there yknahmean but look \\n you need to just let this play out \\n from track one to track whatever however many i put on here \\n you need to just listen to this because look man \\n its the first time canadian down south my man smallz \\n your boy drake you know what it is man \\n so just uhh tune in nigga \\n '\n",
      "'\\n  \\n southern smoke \\n  \\n yeah ho  \\n its drake uh uh ho \\n do that dance do that dance \\n  \\n you special girl dont leave me baby \\n to find another just like you wont be easy baby \\n so at least i know where the ones at \\n baby just promise me if you go that youll come back \\n  \\n look i ought to be thankful because for women i have plenty of love \\n and when it comes to relationships many is stuck \\n look at 50 you make love'\n",
      "'30 you have sex \\n somewhere around the tender age of 20 you fuck \\n or maybe before im here waiting to score \\n its long gone like brothers having a tape in their car \\n instead these girls taking it raw \\n now they tell you to do whatever you want because its safe to explore \\n forget wait till its right now wait till its more  \\n wait till he get gets a deal girl wait till the tour \\n so they dance to attract me advance and attack me \\n to the point that drake compare me'\n",
      "'to anthony mackie \\n but once they have my pants and my khakis \\n i tell them im not one for commitment and romancing exactly \\n because i hate waking up undressed with a dame \\n and guessing her name i hope this shit is destined to change \\n  \\n because you special girl dont leave me baby \\n to find another just like you wont be easy baby \\n so at least i know where the ones at \\n baby just promise me if you go that youll come back \\n  \\n i dont get it its like'\n"
     ]
    }
   ],
   "source": [
    "# create sequence batches from the word_dataset\n",
    "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "print(type(sequences), '\\n')\n",
    "\n",
    "# preview some sequences\n",
    "for item in sequences.take(5):\n",
    "    print(repr(' '.join(idx2word[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text using the `map` method on the batch object to apple a simple function to each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shifting (splitting) function\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # up to but not including the last character\n",
    "    target_text = chunk[1:] # everything except for the firs tcharacter\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "# apply the shifting to create input texts and target texts that comprise of our dataset\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'yo whats goin on this is drake \\n and ima let you know what you about to witness aight \\n this right here is a drake and dj smallz collaboration \\n so im from canada my mans from down south \\n you understand the 1 dj in the south to be exact \\n you heard that at the vmas you heard it wherever he goes \\n my man smallz is out there down south \\n same time reppin for toronto canada yknahmean \\n so this right here what you bout what you listenin to right now \\n is the official'\n",
      "Target data: 'whats goin on this is drake \\n and ima let you know what you about to witness aight \\n this right here is a drake and dj smallz collaboration \\n so im from canada my mans from down south \\n you understand the 1 dj in the south to be exact \\n you heard that at the vmas you heard it wherever he goes \\n my man smallz is out there down south \\n same time reppin for toronto canada yknahmean \\n so this right here what you bout what you listenin to right now \\n is the official southern'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the first few examples of input and target values\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "    print('Target data:', repr(' '.join(idx2word[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, at time step 0, the model receives the index for F (from \"First\") and tries to predict the \"i\" (from \"First\") as the next character.  At the next time step, it does the same thing, but the RNN considers the previous time step context in addition to the current input character (it would consider both \"F\" and \"i\" in trying to predict \"r\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW CELL CAUSES GPU MEMORY SPIKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first few examples of prediction time steps\n",
    "# for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "#     print(f\"Step {i:4d}\")\n",
    "#     print(f\"  input: {input_idx} ({repr(idx2char[input_idx]):s})\")\n",
    "#     print(f\"  expected output: {target_idx} ({repr(idx2char[target_idx]):s})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training *Batches*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` was used to split the text into _sequences_.  But before feeding this data into the model, we must _shuffle_ the data and pack it into _batches_.  The first layer of the model will be a Keras `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "# (TensorFlow data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory.  Instead,\n",
    "# it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset_sb = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be passed into an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions\n",
    "- `tf.keras.layers.GRU`: A type of RNN with size `units = rnn_units` (You can also use an LSTM layer here)\n",
    "- `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary length (number of unique words)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = [vocab_size, embedding_dim, rnn_units]\n",
    "\n",
    "outfile = open(file='model_params', mode='wb')\n",
    "pickle.dump(model_params, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(file='model_params', mode='rb')\n",
    "vocab_size, embedding_dim, rnn_units = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to quickly build the RNN model based on vocab size, embedding dimension, number of RNN units, and batch size\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = vocab_size,\n",
    "        output_dim = embedding_dim,\n",
    "        batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.GRU(\n",
    "        units = rnn_units,\n",
    "        return_sequences = True,\n",
    "        stateful = True,\n",
    "        recurrent_initializer = 'glorot_uniform'\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "rnn = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Model (Without Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 9597) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset_sb.take(1):\n",
    "    example_batch_predictions = rnn(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence length (`seq_length`) was set to `100` but the model can be run on inputs of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           2456832   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 9597)          9836925   \n",
      "=================================================================\n",
      "Total params: 16,232,061\n",
      "Trainable params: 16,232,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we must sample from the output distribution to get actual character indices.  This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "**Note**: It is important to _sample_ from this distribution, since taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # tf.squeeze() removes all size-1 dimensions from the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5816, 2650, 7238, 2160, 4167, 1206, 4934, 5845, 3529, 6255, 6132,\n",
       "       2078, 4346, 7618, 1431, 8902, 5756, 4790, 1609, 7917,  664, 5745,\n",
       "       1380, 4934, 4889, 9370, 6701,  470, 9531, 3868, 7702, 7029, 2726,\n",
       "       5270, 2317, 7709,  573, 3822, 5280, 3049, 6777, 3065, 5522, 3468,\n",
       "       7406, 5075, 3595, 8066, 3461,  293, 6167, 3442, 6027, 5830, 6919,\n",
       "       7507, 7180, 7073, 6417, 4839, 4183, 1332, 6648, 7285, 7737, 8405,\n",
       "       8913, 4946, 2526, 3808, 3269, 1196, 7069, 4521, 2391, 1861, 4393,\n",
       "       7798,  746, 2385, 9204, 2489, 5349, 6896, 8805, 3875, 4707, 2221,\n",
       "       7751,  102, 8486, 3523, 2214, 9301, 2852, 6277, 5370, 6369, 4223,\n",
       "       3466], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "display(sampled_indices)\n",
    "print(len(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by the untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'quit playin \\n girl quit playin \\n you aint love me from the start \\n youre the reason that i feel this way  \\n you broke my fuckin heart \\n and i gave you all my trust but you just tore it all apart \\n now youre all i think about while im layin in the dark \\n  \\n texted me the other night its been too long \\n always tryna figure out if ima move on \\n staring at the ceiling almost all night long \\n probably why i had the time to write this song \\n'\n",
      "\n",
      "Output: 'opposite dry seat dallas imbalance brushed louie oughta glee please phase crucified jag smart catching union old lieutenants chips staringcan bajan oho caramel louie lollipop wolves reclinin arch youngings heartbeat solely roxx echelon milli derek som audit hate mindful featuring reminiscing feh nas george shoot marathon grace stunt generation album pierre gasoline paul orgasm righttighten sinkin scholarships saaaand prettiest lipstick impression calls razor sender sop testosterone unlucky lovey door harvard forces brr russ keeper digital concern jeopardizing speeches bass dig weekends dodgy mofucker rich tweet heat layin dealin souls 6am thorough glancin dead widebody essence poetry monstars praises inkin gentleman'\n"
     ]
    }
   ],
   "source": [
    "print(f'Input: {repr(\" \".join(idx2word[input_example_batch[0]]))}\\n')\n",
    "print(f'Output: {repr(\" \".join(idx2word[sampled_indices]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a classification problem: **Given the previous RNN state, and the input at this time step, predict the class of the next character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching an Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because the model returns logits, we need to set the `from_logits` flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to obtain the loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 9597)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       9.169176\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method.  Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "\n",
    "# create checkpoints-saving object\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    monitor = 'loss',\n",
    "    save_best_only = True,\n",
    "    mode = 'min',\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of desired epochs\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 11s 367ms/step - loss: 6.7530 - accuracy: 0.1027\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 11s 378ms/step - loss: 5.9644 - accuracy: 0.1338\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 11s 374ms/step - loss: 5.6714 - accuracy: 0.1562\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 11s 376ms/step - loss: 5.4602 - accuracy: 0.1674\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 11s 377ms/step - loss: 5.2914 - accuracy: 0.1735\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 11s 375ms/step - loss: 5.1571 - accuracy: 0.1832\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 11s 374ms/step - loss: 5.0504 - accuracy: 0.1909\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 11s 374ms/step - loss: 4.9390 - accuracy: 0.1986\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 11s 375ms/step - loss: 4.8235 - accuracy: 0.2089\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 11s 375ms/step - loss: 4.7146 - accuracy: 0.2171\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 11s 385ms/step - loss: 4.6001 - accuracy: 0.2262\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 11s 395ms/step - loss: 4.4892 - accuracy: 0.2341\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 11s 393ms/step - loss: 4.3746 - accuracy: 0.2439\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 11s 392ms/step - loss: 4.2511 - accuracy: 0.2518\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 11s 389ms/step - loss: 4.1224 - accuracy: 0.2619\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 11s 386ms/step - loss: 3.9853 - accuracy: 0.2742\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 12s 398ms/step - loss: 3.8437 - accuracy: 0.2890\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 11s 390ms/step - loss: 3.6938 - accuracy: 0.3054\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 11s 396ms/step - loss: 3.5307 - accuracy: 0.3247\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 11s 393ms/step - loss: 3.3741 - accuracy: 0.3456\n",
      "Epoch 21/50\n",
      " 9/29 [========>.....................] - ETA: 6s - loss: 3.1420 - accuracy: 0.3840"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf22\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training!\n",
    "history = rnn.fit(\n",
    "    x = dataset_sb,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generating Text (Making Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the Latest Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch size 1 (for simplicity)\n",
    "- because of the way the RNN state is passed from time step to time step, the model only accepts a fixed batch size once built\n",
    "- **to run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the last checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\checkpoint'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the file in the working directory that contains the most recent checkpoint\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a new RNN model instance\n",
    "rnn_cp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# load the saved weights from the checkpoint into the new model instance\n",
    "rnn_cp.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# build the model with a new input shape\n",
    "rnn_cp.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            2456832   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 9597)           9836925   \n",
      "=================================================================\n",
      "Total params: 16,232,061\n",
      "Trainable params: 16,232,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start by choosing a start string, initializing the RNN state and setting the number of characters to generate\n",
    "- get the prediction distribution of the next character using the start string and the RNN state\n",
    "- then, use a categorical distribution to calculate the index of the predicted character\n",
    "- use this predicted character as our next input to the model\n",
    "- the RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character\n",
    "- after predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prediction function\n",
    "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
    "    \n",
    "    # num of chars to generate\n",
    "    num_generate = num_generate\n",
    "    \n",
    "    # vectorizing the start string to numbers\n",
    "    input_eval = [word2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input=input_eval, axis=0) # returns a tensor with a length-1 axis inserted at index `axis`\n",
    "    \n",
    "    # empty string to store results\n",
    "    text_generated = list()\n",
    "    \n",
    "    # \"temperature\"\n",
    "    # low temperature results in more predictable text,\n",
    "    # high temperature results in more surprising text.\n",
    "    # feel free to experiment with this parameter\n",
    "    temperature = 1.0\n",
    "    \n",
    "    # the batch size was defined when we loaded model weights from training\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # use a categorical distribution to predict the character returned by the model\n",
    "        preidctions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "    \n",
    "    return(' '.join(start_string + text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# text generation!\n",
    "print(generate_text(rnn_cp, start_string=['bruh', 'no', 'cap'], num_generate=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
