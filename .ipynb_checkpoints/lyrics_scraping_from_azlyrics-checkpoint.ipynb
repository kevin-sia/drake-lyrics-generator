{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AZLyrics Lyrics Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look to gather lyrics from a single artist from [AZLyrics.com](https://www.azlyrics.com), then train a recurrent neural network (RNN) model on those lyrics, with the goal of generating lyrics based on some input text.\n",
    "\n",
    "RNN models are meant to be trained on _sequential_ data (time series, language, etc.), so our lyrics data should be arranged in a sequential manner as best as possible!  For language RNN models, models are simpler if they predict _characters_.  Predicting _words_ requires embedding the text into a significantly larger dimensional space, thus increasing computational demand ro process the data, but the predictions may become more meaningful.  A larger neural network architecture may improve predictions with this higher-dimensional data, also increasing computational demand.\n",
    "\n",
    "The data used by this model is a single text file that contains all of the text we want to train on.  But, how exactly do we make this data sequential?  _Lyrics_ (and language in general) are indeed sequential, _but only within one song_, i.e. there is no meaningful order in a sequence that contains words/characters from the end of one song and words/characters from the beginning of another.  The best we could do is arrange the songs in chronological order - maybe there is some meaning between two consecutive songs on an album, but there is probably less meaning between the last song of one album and the first song of the next album.  **Doing this may require iterative string appending in Python, which could eat up RAM, but probably won't be an issue considering we aren't working with too much data here.  The alternative is to save each song's lyrics to their own file, then concatenate them after scraping is complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KIND OF COUNTERINTUITIVE TO THE WAY THE SCRIPT IS WRITTEN? i.e. if you arrange songs by when they were saved, then it'll be out of order if you run the script in parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF PYTHON CONCATENATION,\n",
    "\n",
    "- don't need song_names\n",
    "- no need for saving individual files\n",
    "- no need for bash concatenation script\n",
    "- save a very large concatenated string as a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX ERROR COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"UNABLE TO BE SAVED\" AND THEN \"SAVED\"???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE LOG ONLY MAKES SENSE IF LYRICS ARE MEANT TO BE SCRAPED IN ONE GO.  ALSO IT WOULD HAVE TO BE LOCATED OUTSIDE OF THE ARTIST'S FOLDER IF RUNNING CONCATENATE THERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will serve as a development tool for a script that scrapes all of a single artist's lyrics from AZLyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-632e98f68c4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from string import ascii_lowercase, digits, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for formatting song names into song file names\n",
    "\n",
    "def song_name_for_file(song_name):\n",
    "    '''\n",
    "    Formats a song name string into another string suitable for file/directory naming.\n",
    "    '''\n",
    "    \n",
    "    # make lowercase\n",
    "    song_file_name = song_name.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    for punc in punctuation:\n",
    "        song_file_name = song_file_name.replace(punc, '')\n",
    "\n",
    "    # replace spaces with underscores\n",
    "    song_file_name = song_file_name.replace(' ', '_')\n",
    "    \n",
    "    return song_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for formatting song file names into song url names\n",
    "# retaining only digits and lowercase letters\n",
    "\n",
    "def song_name_for_url(song_name):\n",
    "    \n",
    "    # make song name lowercase\n",
    "    song_name = song_name.lower()\n",
    "    \n",
    "    # empty string to append characters to\n",
    "    song_name_url = str()\n",
    "    \n",
    "    # loop through each character of the song name\n",
    "    for char in song_name:\n",
    "        \n",
    "        # if the character is either a lowercase letter or a digit,\n",
    "        if (char in ascii_lowercase) or (char in digits):\n",
    "            \n",
    "            # append it to the new string\n",
    "            song_name_url += char\n",
    "            \n",
    "    return song_name_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input an artist page URL from AZLyrics (e.g. https://www.azlyrics.com/d/drake.html).\n",
      "https://www.azlyrics.com/d/drake.html\n"
     ]
    }
   ],
   "source": [
    "# input artist page from azlyrics\n",
    "artist_url = input('\\nPlease input an artist page URL from AZLyrics (e.g. https://www.azlyrics.com/d/drake.html).\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 37), match='https://www.azlyrics.com/d/drake.html'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if valid azlyrics artist page URL\n",
    "re.match(pattern='https:\\/\\/www\\.azlyrics\\.com\\/([a-z]|[1][9])\\/.+\\.html', string=artist_url)\n",
    "# if this returns a re.Match object, then there is a match, and the object has a boolean True value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send GET request\n",
    "r_songs = requests.get(artist_url)\n",
    "\n",
    "# beautify the request response\n",
    "soup_songs = BeautifulSoup(r_songs.content, features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obtaining lyrics from songs by Drake.\n"
     ]
    }
   ],
   "source": [
    "# obtain artist name\n",
    "artist_name = soup_songs.find_all('strong')[0].text[:-7]\n",
    "artist_dir_name = artist_name.lower().replace(' ', '_')\n",
    "print(f'\\nObtaining lyrics from songs by {artist_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"drake\" directory created in the working directory.\n"
     ]
    }
   ],
   "source": [
    "# create directory to store lyrics in\n",
    "if not os.path.exists(artist_dir_name):\n",
    "    os.mkdir(artist_dir_name)\n",
    "    print(f'\\n\"{artist_dir_name}\" directory created in the working directory.')\n",
    "\n",
    "# print message if it already exists\n",
    "else:\n",
    "    print(f'\\n\"{artist_dir_name}\" directory already exists in the working directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDED CODE TO OBTAIN SONG NAMES FROM ARTIST PAGE (TO BE USED IN SCRAPING SECTION TO BYPASS SONGS THAT HAVE ALREADY BEEN SCRAPED)\n",
    "\n",
    "# NOTE THAT THIS SEARCH ALGORITHM WILL BE A LITTLE $n^2$-ish ACROSS $n$ SONGS (BUT NOT A BIG DEAL SINCE THE MOST PROLIFIC ARTISTS PROBABLY ONLY HAVE A FEW HUNDREDS OF SONGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store href links and song names in\n",
    "page_urls = list()\n",
    "song_names = list()\n",
    "\n",
    "# parse through the 'a' elements and obtain the href links\n",
    "for link in soup_songs.find_all('a'):\n",
    "\n",
    "    # href link\n",
    "    href = link.get('href')\n",
    "\n",
    "    # check if this href link is a string\n",
    "    if isinstance(href, str):\n",
    "\n",
    "        # if the string is a valid lyrics page URL (kind of hack-y/hard-coded method here)\n",
    "        if link.get('href')[:9] == '../lyrics':\n",
    "\n",
    "            # then append the href link to the page links list\n",
    "            page_urls.append(link.get('href'))\n",
    "            \n",
    "            # get song name\n",
    "            song_name = link.text\n",
    "            \n",
    "            # append song name to list\n",
    "            song_names.append(song_name)\n",
    "\n",
    "# remove duplicates from both URLs list and song name list\n",
    "# list-to-dictionary-to-list method which feels a bit hack-y but works\n",
    "page_urls = list(dict.fromkeys(page_urls))\n",
    "song_names = list(dict.fromkeys(song_names))\n",
    "\n",
    "# recreate the song names list but convert each name into formats useable for URLs\n",
    "# note that we are keeping the same order of song names by basing the URL song names list off of the pure song names list\n",
    "song_url_names = list()\n",
    "for song_name in song_names:\n",
    "    song_url_names.append(song_name_for_url(song_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The song name in the link appears to be only the digits and lowercase letters from the song name (see [Bon Iver lyrics](https://www.azlyrics.com/b/boniver.html) for examples).  Need to make a new list of song names as they might appear in the URLs.  Note that this is not 100% confident (just a posteriori)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval of URLs is complete.  323 songs obtained.\n",
      "Sleep timer between API requests is set to 10 seconds.\n",
      "Lyrics scraping will take 53.8 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to store full lyrics page URLs in\n",
    "page_urls_complete = []\n",
    "\n",
    "# loop through each href link from the previous list\n",
    "for link in page_urls:\n",
    "\n",
    "    # attach the azlyrics URL to the rest of the link\n",
    "    page_urls_complete.append('https://www.azlyrics.com' + link[2:])\n",
    "\n",
    "# number of songs obtained\n",
    "num_songs = len(page_urls_complete)\n",
    "\n",
    "# completion message for URL retrieval\n",
    "print(f'\\nRetrieval of URLs is complete.  {num_songs} songs obtained.')\n",
    "print(f'Sleep timer between API requests is set to 10 seconds.')\n",
    "print(f'Lyrics scraping will take {round(num_songs*10/60, 1)} minutes.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for \"Intro\" lyrics already exists in the drake directory.\n",
      "File for \"Special\" lyrics already exists in the drake directory.\n",
      "Lyrics for Do What You Do saved to drake/dowhatyoudo.txt.\n",
      "Lyrics for Money (Remix) saved to drake/moneyremix.txt.\n",
      "Lyrics for A.M. 2 P.M. saved to drake/am2pm.txt.\n",
      "Lyrics for City Is Mine saved to drake/cityismine.txt.\n",
      "Lyrics for Bad Meanin' Good saved to drake/badmeaningood.txt.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d328c6d33612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Lyrics for {song_names[i]} unable to be written (e.g. encoding issues).  {lyrics_file_name} saved as an empty text file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TEST THE SCRAPING LOOP WITH A BYPASS IF THE FILE ALREADY EXISTS ###\n",
    "\n",
    "# loop through every song\n",
    "for i in range(len(song_names)):\n",
    "    \n",
    "    # create a potential lyrics file name\n",
    "    lyrics_file_name = f'{artist_dir_name}/{song_url_names[i]}.txt'\n",
    "    \n",
    "    # if the lyrics file already exists,\n",
    "    if os.path.exists(lyrics_file_name):\n",
    "        \n",
    "        # print a message saying that it already exists in the artist director\n",
    "        print(f'File for \"{song_names[i]}\" lyrics already exists in the {artist_dir_name} directory.')\n",
    "    \n",
    "    # if the lyrics file DOESN'T exist,\n",
    "    else:\n",
    "        \n",
    "        # loop through every lyrics page URL\n",
    "        for url in page_urls_complete:\n",
    "            \n",
    "            # check if the song URL name is in the lyrics page URL\n",
    "            if song_url_names[i] in url:\n",
    "                \n",
    "                # send GET request\n",
    "                r_lyrics = requests.get(url)\n",
    "\n",
    "                # beautify the request response\n",
    "                soup_lyrics = BeautifulSoup(r_lyrics.content, features='html.parser')\n",
    "\n",
    "                # find all div elements\n",
    "                divs = soup_lyrics.find_all('div')\n",
    "\n",
    "                # found by trial and error - lyrics are located at the 21st element (index 20) of the list returned by\n",
    "                # the .find_all('div') method on each azlyrics lyrics page\n",
    "                lyrics = divs[20].text\n",
    "                \n",
    "                # remove any square brackets (which, on azlyrics, indicate which person is singing/rapping/talking)\n",
    "                lyrics = re.sub('\\[.*\\]', '', lyrics)\n",
    "\n",
    "                # remove line breaks from beginning of lyrics\n",
    "                while lyrics[0] == '\\n' or lyrics[0] == '\\r':\n",
    "                    lyrics = lyrics[1:]\n",
    "\n",
    "                # replace triple line breaks with double line breaks until there are no remaining triple line breaks\n",
    "                while '\\n\\n\\n' in lyrics:\n",
    "                    lyrics = lyrics.replace('\\n\\n\\n', '\\n\\n')\n",
    "\n",
    "                # remove line breaks from end of lyrics\n",
    "                while lyrics[-1] == '\\n' or lyrics[-1] == '\\r':\n",
    "                    lyrics = lyrics[:-1]\n",
    "\n",
    "                # attach triple line break to end of text to indicate end of song\n",
    "                lyrics_cleaned = lyrics + '\\n\\n\\n'\n",
    "\n",
    "                # open/create an empty text file\n",
    "                with open(lyrics_file_name, 'w') as lyrics_file:\n",
    "\n",
    "                    # adding this exception for when characters cannot be encoded\n",
    "                    # e.g. the prime symbol in https://www.azlyrics.com/lyrics/drake/roundofapplause.html\n",
    "                    try:\n",
    "\n",
    "                        # write cleaned lyrics into a text file in the artist directory\n",
    "                        lyrics_file.write(lyrics_cleaned)\n",
    "\n",
    "                        # completion message for saving of song's lyrics to a text file\n",
    "                        print(f'Lyrics for {song_names[i]} saved to {lyrics_file_name}.')\n",
    "\n",
    "                    except UnicodeEncodeError:\n",
    "\n",
    "                        # message if text file can't be saved\n",
    "                        print(f'Lyrics for {song_names[i]} unable to be written (e.g. encoding issues).  {lyrics_file_name} saved as an empty text file.')\n",
    "        \n",
    "        time.sleep(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_page_url = page_links_complete[0]\n",
    "lyrics_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_page_url = 'https://www.azlyrics.com/lyrics/boniver/calgary.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEND GET REQUEST TO API ###\n",
    "\n",
    "# send GET request\n",
    "r_lyrics = requests.get(lyrics_page_url)\n",
    "\n",
    "# beautify the request response\n",
    "soup_lyrics = BeautifulSoup(r_lyrics.content, features='html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW LINE BREAKS CLEANING ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBTAIN SONG NAME AND SONG FILE NAME ###\n",
    "\n",
    "# obtain song name from song page, drop single quotation marks\n",
    "song_name = soup_lyrics.find_all('b')[1].text.replace('\\\"', '')\n",
    "\n",
    "# remove punctuation from song name to create a song file name\n",
    "song_file_name = song_name.lower()\n",
    "for punc in punctuation:\n",
    "    song_file_name = song_file_name.replace(punc, '')\n",
    "    \n",
    "# replace spaces with underscores\n",
    "song_file_name = song_file_name.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBTAIN LYRICS ###\n",
    "\n",
    "# find all div elements\n",
    "divs = soup_lyrics.find_all('div')\n",
    "\n",
    "# found by trial and error - lyrics are located at the 21st element (index 20) of the list returned by\n",
    "# the .find_all('div') method on each azlyrics lyrics page\n",
    "lyrics = divs[20].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING THE LYRICS ###\n",
    "\n",
    "# remove any square brackets (which, on azlyrics, indicate which person is singing/rapping/talking)\n",
    "lyrics = re.sub('\\[.*\\]', '', lyrics)\n",
    "    \n",
    "# remove line breaks from beginning of lyrics\n",
    "while lyrics[0] == '\\n' or lyrics[0] == '\\r':\n",
    "    lyrics = lyrics[1:]\n",
    "\n",
    "# replace triple line breaks with double line breaks until there are no remaining triple line breaks\n",
    "while '\\n\\n\\n' in lyrics:\n",
    "    lyrics = lyrics.replace('\\n\\n\\n', '\\n\\n')\n",
    "\n",
    "# remove line breaks from end of lyrics\n",
    "while lyrics[-1] == '\\n' or lyrics[-1] == '\\r':\n",
    "    lyrics = lyrics[:-1]\n",
    "\n",
    "# attach triple line break to end of text to indicate end of song\n",
    "lyrics_cleaned = lyrics + '\\n\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# file name/path for lyrics file\n",
    "lyrics_file_name = f'{artist_dir_name}/{song_file_name}.txt'\n",
    "\n",
    "# write cleaned lyrics to a text file in the artist directory that was created\n",
    "if not os.path.exists(lyrics_file_name):\n",
    "    \n",
    "    # open/create an empty text file\n",
    "    with open(lyrics_file_name, 'w') as lyrics_file:\n",
    "        \n",
    "        # adding this exception for when characters cannot be encoded\n",
    "        # e.g. the prime symbol in https://www.azlyrics.com/lyrics/drake/roundofapplause.html\n",
    "        try:\n",
    "            \n",
    "            # write cleaned lyrics into a text file in the artist directory\n",
    "            lyrics_file.write(lyrics_cleaned)\n",
    "            \n",
    "            # completion message for saving of song's lyrics to a text file\n",
    "            print(f'Lyrics for {song_name} saved to {lyrics_file_name}.')\n",
    "            \n",
    "        except UnicodeEncodeError:\n",
    "            \n",
    "            # message if text file can't be saved\n",
    "            print(f'Lyrics for {song_name} unable to be written (e.g. encoding issues).  {lyrics_file_name} saved as an empty text file.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print(f'Lyrics for {song_name} already exist in {lyrics_file_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input an artist page URL from AZLyrics (e.g. https://www.azlyrics.com/d/drake.html).\n",
      "https://www.azlyrics.com/b/boniver.html\n",
      "\n",
      "Obtaining lyrics from songs by Bon Iver.\n",
      "\n",
      "\"bon_iver\" directory already exists in the working directory.\n",
      "\n",
      "Retrieval of URLs is complete.  51 songs obtained.\n",
      "Sleep timer between API requests is set to 10 seconds.\n",
      "Lyrics scraping will take 8.5 minutes.\n",
      "\n",
      "File for \"Flume\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Lump Sum\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Skinny Love\" lyrics already exists in the bon_iver directory.\n",
      "File for \"The Wolves (Act I And II)\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Blindsided\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Creature Fear\" lyrics already exists in the bon_iver directory.\n",
      "File for \"For Emma\" lyrics already exists in the bon_iver directory.\n",
      "File for \"re: Stacks\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Wisconsin\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Blood Bank\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Beach Baby\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Babys\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Woods\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Perth\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Minnesota, WI\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Holocene\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Towers\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Michicant\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Hinnom, TX\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Wash.\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Calgary\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Beth / Rest\" lyrics already exists in the bon_iver directory.\n",
      "File for \"22 (OVER S∞∞N)\" lyrics already exists in the bon_iver directory.\n",
      "File for \"10 d E A T h b R E a s T ⚄ ⚄\" lyrics already exists in the bon_iver directory.\n",
      "File for \"715 - CR∑∑KS\" lyrics already exists in the bon_iver directory.\n",
      "File for \"33 “GOD”\" lyrics already exists in the bon_iver directory.\n",
      "File for \"29 #Strafford APTS\" lyrics already exists in the bon_iver directory.\n",
      "File for \"666 ʇ\" lyrics already exists in the bon_iver directory.\n",
      "File for \"21 M◊◊N WATER\" lyrics already exists in the bon_iver directory.\n",
      "File for \"8 (circle)\" lyrics already exists in the bon_iver directory.\n",
      "File for \"____45_____\" lyrics already exists in the bon_iver directory.\n",
      "File for \"00000 Million\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Yi\" lyrics already exists in the bon_iver directory.\n",
      "File for \"iMi\" lyrics already exists in the bon_iver directory.\n",
      "File for \"We\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Holyfields,\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Hey, Ma\" lyrics already exists in the bon_iver directory.\n",
      "File for \"U (Man Like)\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Naeem\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Jelmore\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Faith\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Marion\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Salem\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Sh'Diah\" lyrics already exists in the bon_iver directory.\n",
      "File for \"RABi\" lyrics already exists in the bon_iver directory.\n",
      "File for \"AUATC\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Brackett, Wi\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Heavenly Father\" lyrics already exists in the bon_iver directory.\n",
      "File for \"I Can't Make You Love Me\" lyrics already exists in the bon_iver directory.\n",
      "File for \"PDLIF\" lyrics already exists in the bon_iver directory.\n",
      "File for \"Roslyn\" lyrics already exists in the bon_iver directory.\n",
      "\n",
      "Lyrics for 51 songs have been saved to the bon_iver directory.\n",
      "Lyrics of 0 songs were unabled to be saved.  More info can be found at bon_iver/__SCRAPING_LOG.txt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### FULL SCRIPT ###\n",
    "\n",
    "\n",
    "\n",
    "# helper function for formatting song file names into song url names (retain only digits and lowercase letters)\n",
    "def song_name_for_url(song_name):\n",
    "    \n",
    "    # make song name lowercase\n",
    "    song_name = song_name.lower()\n",
    "    \n",
    "    # empty string to append characters to\n",
    "    song_name_url = str()\n",
    "    \n",
    "    # loop through each character of the song name\n",
    "    for char in song_name:\n",
    "        \n",
    "        # if the character is either a lowercase letter or a digit,\n",
    "        if (char in ascii_lowercase) or (char in digits):\n",
    "            \n",
    "            # append it to the new string\n",
    "            song_name_url += char\n",
    "            \n",
    "    return song_name_url\n",
    "\n",
    "\n",
    "\n",
    "# input artist page from azlyrics\n",
    "artist_url = input('\\nPlease input an artist page URL from AZLyrics (e.g. https://www.azlyrics.com/d/drake.html).\\nURL: ')\n",
    "\n",
    "# check if valid azlyrics artist page URL\n",
    "if re.match(pattern='https:\\/\\/www\\.azlyrics\\.com\\/([a-z]|[1][9])\\/.+\\.html', string=artist_url):\n",
    "    \n",
    "    \n",
    "    ### OBTAINING ARTIST NAME AND CREATING DIRECTORY FOR ARTIST'S LYRICS ###\n",
    "    \n",
    "    # send GET request\n",
    "    r_songs = requests.get(artist_url)\n",
    "\n",
    "    # beautify the request response\n",
    "    soup_songs = BeautifulSoup(r_songs.content, features='html.parser')\n",
    "    \n",
    "    # obtain artist name\n",
    "    artist_name = soup_songs.find_all('strong')[0].text[:-7]\n",
    "    artist_dir_name = artist_name.lower().replace(' ', '_')\n",
    "    print(f'\\nObtaining lyrics from songs by {artist_name}.')\n",
    "    \n",
    "    # create directory to store lyrics in\n",
    "    if not os.path.exists(artist_dir_name):\n",
    "        os.mkdir(artist_dir_name)\n",
    "        print(f'\\n\"{artist_dir_name}\" directory created in the working directory.')\n",
    "\n",
    "    # print message if it already exists\n",
    "    else:\n",
    "        print(f'\\n\"{artist_dir_name}\" directory already exists in the working directory.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### OBTAINING SONG NAMES AND LYRICS PAGE URLs ###\n",
    "    \n",
    "    # empty lists to store href links and song names in\n",
    "    page_urls = list()\n",
    "    song_names = list()\n",
    "\n",
    "    # parse through the 'a' elements and obtain the href links\n",
    "    for link in soup_songs.find_all('a'):\n",
    "\n",
    "        # href link\n",
    "        href = link.get('href')\n",
    "\n",
    "        # check if this href link is a string\n",
    "        if isinstance(href, str):\n",
    "\n",
    "            # if the string is a valid lyrics page URL (kind of hack-y/hard-coded method here)\n",
    "            if link.get('href')[:9] == '../lyrics':\n",
    "\n",
    "                # then append the href link to the page links list\n",
    "                page_urls.append(link.get('href'))\n",
    "\n",
    "                # get song name\n",
    "                song_name = link.text\n",
    "\n",
    "                # append song name to list\n",
    "                song_names.append(song_name)\n",
    "\n",
    "    # remove duplicates from both links and song name lists, and sort them\n",
    "    page_urls = list(dict.fromkeys(page_urls))\n",
    "    song_names = list(dict.fromkeys(song_names))\n",
    "\n",
    "    # recreate the song names list but convert each name into formats useable for URLs\n",
    "    song_url_names = list()\n",
    "    for song_name in song_names:\n",
    "        song_url_names.append(song_name_for_url(song_name))\n",
    "    \n",
    "    # empty list to store full lyrics page URLs in\n",
    "    page_urls_complete = []\n",
    "\n",
    "    # loop through each href link from the previous list\n",
    "    for link in page_urls:\n",
    "\n",
    "        # attach the azlyrics URL to the rest of the link\n",
    "        page_urls_complete.append('https://www.azlyrics.com' + link[2:])\n",
    "\n",
    "    # number of songs obtained\n",
    "    num_songs = len(song_names)\n",
    "\n",
    "    # completion message for URL retrieval\n",
    "    print(f'\\nRetrieval of URLs is complete.  {num_songs} songs obtained.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### SCRAPING LYRICS ###\n",
    "    \n",
    "    # set timer between API quests\n",
    "    sleep_timer = 10\n",
    "    print(f'Sleep timer between API requests is set to {sleep_timer} seconds.')\n",
    "    print(f'Lyrics scraping will take {round(num_songs*sleep_timer/60, 1)} minutes at most.\\n')\n",
    "\n",
    "    # loop through every song\n",
    "    for i in range(len(song_names)):\n",
    "\n",
    "        # create a potential lyrics file name\n",
    "        lyrics_file_name = f'{artist_dir_name}/{song_url_names[i]}.txt'\n",
    "\n",
    "        # if the lyrics file already exists,\n",
    "        if os.path.exists(lyrics_file_name):\n",
    "\n",
    "            # print a message saying that it already exists in the artist director\n",
    "            print(f'File for \"{song_names[i]}\" lyrics already exists in the {artist_dir_name} directory.')\n",
    "\n",
    "        # if the lyrics file DOESN'T exist,\n",
    "        else:\n",
    "\n",
    "            # loop through every lyrics page URL\n",
    "            for url in page_urls_complete:\n",
    "\n",
    "                # check if the song URL name is in the lyrics page URL\n",
    "                if song_url_names[i] in url:\n",
    "\n",
    "                    # send GET request\n",
    "                    r_lyrics = requests.get(url)\n",
    "\n",
    "                    # beautify the request response\n",
    "                    soup_lyrics = BeautifulSoup(r_lyrics.content, features='html.parser')\n",
    "\n",
    "                    # find all div elements\n",
    "                    divs = soup_lyrics.find_all('div')\n",
    "\n",
    "                    # found by trial and error - lyrics are located at the 21st element (index 20) of the list returned by\n",
    "                    # the .find_all('div') method on each azlyrics lyrics page\n",
    "                    lyrics = divs[20].text\n",
    "\n",
    "                    # remove any square brackets (which, on azlyrics, indicate which person is singing/rapping/talking)\n",
    "                    lyrics = re.sub('\\[.*\\]', '', lyrics)\n",
    "\n",
    "                    # remove line breaks from beginning of lyrics\n",
    "                    while lyrics[0] == '\\n' or lyrics[0] == '\\r':\n",
    "                        lyrics = lyrics[1:]\n",
    "\n",
    "                    # replace triple line breaks with double line breaks until there are no remaining triple line breaks\n",
    "                    while '\\n\\n\\n' in lyrics:\n",
    "                        lyrics = lyrics.replace('\\n\\n\\n', '\\n\\n')\n",
    "\n",
    "                    # remove line breaks from end of lyrics\n",
    "                    while lyrics[-1] == '\\n' or lyrics[-1] == '\\r':\n",
    "                        lyrics = lyrics[:-1]\n",
    "\n",
    "                    # attach triple line break to end of text to indicate end of song\n",
    "                    lyrics_cleaned = lyrics + '\\n\\n\\n'\n",
    "                    \n",
    "                    # create empty list for song names whose lyrics can't be saved (in the `except` statement)\n",
    "                    log = list()\n",
    "                    \n",
    "                    # error counter\n",
    "                    error_counter = 0\n",
    "\n",
    "                    # open/create an empty text file\n",
    "                    with open(lyrics_file_name, 'w') as lyrics_file:\n",
    "\n",
    "                        # adding this exception for when characters cannot be encoded\n",
    "                        # e.g. the prime symbol in https://www.azlyrics.com/lyrics/drake/roundofapplause.html\n",
    "                        try:\n",
    "\n",
    "                            # write cleaned lyrics into a text file in the artist directory\n",
    "                            lyrics_file.write(lyrics_cleaned)\n",
    "\n",
    "                            # completion message for saving of song's lyrics to a text file\n",
    "                            print(f'Lyrics for {song_names[i]} saved to {lyrics_file_name}.')\n",
    "\n",
    "                        except UnicodeEncodeError:\n",
    "                            \n",
    "                            error_counter += 1\n",
    "\n",
    "                            # message if text file can't be saved\n",
    "                            print(f'Lyrics for {song_names[i]} unable to be written (e.g. encoding issues).  {lyrics_file_name} saved as an empty text file.')\n",
    "                            \n",
    "                            # append song name to log list\n",
    "                            log.append(song_names[i])\n",
    "            \n",
    "            # add sleep timer to prevent spamming GET requests\n",
    "            time.sleep(sleep_timer)\n",
    "    \n",
    "    # create log file text\n",
    "    log_str = f'Lyrics for the following {artist_name} songs were unable to be written:\\n\\n'\n",
    "    for song in log:\n",
    "        log_str += f'- {song}\\n'\n",
    "    \n",
    "    # write log file\n",
    "    log_file_name = f'{artist_dir_name}/__SCRAPING_LOG.txt'\n",
    "    with open(log_file_name, 'w') as log_file:\n",
    "        log_file.write(log_str)\n",
    "\n",
    "    # summary statements\n",
    "    print(f'\\nLyrics for {len(song_names) - error_counter} songs have been saved to the {artist_dir_name} directory.')\n",
    "    print(f'Lyrics of {error_counter} songs were unabled to be saved.  More info can be found at {log_file_name}.\\n')\n",
    "    \n",
    "\n",
    "# if input URL is not a valid AZLyrics artist page\n",
    "else:\n",
    "    \n",
    "    print('Please input a valid artist page URL from AZLyrics (e.g. https://www.azlyrics.com/d/drake.html).\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fri Dec 11 02:36:16 2020'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.ctime(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
