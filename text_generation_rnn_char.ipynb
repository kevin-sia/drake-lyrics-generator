{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow text generation with an RNN tutorial](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow GPU memory growth must be limited to allow model to train (was having issues without doing this).  Code in below cell borrowed from the [TensorFlow documentation](https://www.tensorflow.org/guide/gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting GPU memory growth\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # enabling memory growth\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPU')\n",
    "    except RuntimeError as e:\n",
    "        # memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text must all be in a single `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download file\n",
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# # open the file\n",
    "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# # data type?\n",
    "# print(type(text))\n",
    "\n",
    "# # number of characters\n",
    "# print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "text = str(open('data/drake_lyrics.txt', 'r').read())\n",
    "\n",
    "# data type?\n",
    "print(type(text))\n",
    "\n",
    "# number of characters\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek into file\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unique characters in file\n",
    "vocab = sorted(set(text))\n",
    "print(type(vocab))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is **character vectorization**.  Word vectorization would probably make more coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# reverse the map - use this to specify an index to obtain a character\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# entire text document represented in the above character-to-indices mapping\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# sample\n",
    "print(f'\"{text[:13]}\" ---- characters mapped to int ---- > {text_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Examples & Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model input**: sequence of characters\n",
    "\n",
    "**model output (prediction)**: the following character at each step (based on previous characters in the sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the text into **example sequences**.  Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "**For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.**\n",
    "\n",
    "So, break the text into chunks of `seq_length + 1`.  e.g. if `seq_length` is 4 and our text is \"Hello\", the input sequence would be \"Hell\" and the target sequence would be \"ello\".\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices` converts the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length (in number of characters) desired for single input\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1) # floored division\n",
    "\n",
    "# create training examples/targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# data type of train examples/targets\n",
    "print(type(char_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview training examples as characters (using the indices in char_dataset)\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()]) # .numpy() converts into numpy data format (in this case, a numpy integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `batch` method on `char_dataset` (type `tensorflow.python.data.ops.dataset_ops.TensorSliceDataset`) to convert the individual characters to sequences of the desired size (`seq_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequence batches from the char_dataset\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "print(type(sequences), '\\n')\n",
    "\n",
    "# preview some sequences\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text using the `map` method on the batch object to apple a simple function to each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shifting (splitting) function\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # up to but not including the last character\n",
    "    target_text = chunk[1:] # everything except for the firs tcharacter\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the shifting to create input texts and target texts that comprise of our dataset\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the first few examples of input and target values\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, at time step 0, the model receives the index for F (from \"First\") and tries to predict the \"i\" (from \"First\") as the next character.  At the next time step, it does the same thing, but the RNN considers the previous time step context in addition to the current input character (it would consider both \"F\" and \"i\" in trying to predict \"r\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW CELL CAUSES GPU MEMORY SPIKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first few examples of prediction time steps\n",
    "# for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "#     print(f\"Step {i:4d}\")\n",
    "#     print(f\"  input: {input_idx} ({repr(idx2char[input_idx]):s})\")\n",
    "#     print(f\"  expected output: {target_idx} ({repr(idx2char[target_idx]):s})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training *Batches*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` was used to split the text into _sequences_.  But before feeding this data into the model, we must _shuffle_ the data and pack it into _batches_.  The first layer of the model will be a Keras `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "# (TensorFlow data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory.  Instead,\n",
    "# it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset_sb = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset_sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be passed into an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions\n",
    "- `tf.keras.layers.GRU`: A type of RNN with size `units = rnn_units` (You can also use an LSTM layer here)\n",
    "- `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary length (number of characters)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to quickly build the RNN model based on vocab size, embedding dimension, number of RNN units, and batch size\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = vocab_size,\n",
    "        output_dim = embedding_dim,\n",
    "        batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.GRU(\n",
    "        units = rnn_units,\n",
    "        return_sequences = True,\n",
    "        stateful = True,\n",
    "        recurrent_initializer = 'glorot_uniform'\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "rnn = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Model (Without Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset_sb.take(1):\n",
    "    example_batch_predictions = rnn(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence length (`seq_length`) was set to `100` but the model can be run on inputs of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we must sample from the output distribution to get actual character indices.  This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "**Note**: It is important to _sample_ from this distribution, since taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # tf.squeeze() removes all size-1 dimensions from the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sampled_indices)\n",
    "print(len(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by the untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input: {repr(\"\".join(idx2char[input_example_batch[0]]))}\\n')\n",
    "print(f'Output: {repr(\"\".join(idx2char[sampled_indices]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a classification problem: **Given the previous RNN state, and the input at this time step, predict the class of the next character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching an Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because the model returns logits, we need to set the `from_logits` flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to obtain the loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method.  Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "\n",
    "# create checkpoints-saving object\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    monitor = 'loss',\n",
    "    save_best_only = True,\n",
    "    mode = 'min',\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of desired epochs\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training!\n",
    "history = rnn.fit(\n",
    "    x = dataset_sb,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text (Making Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the Latest Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch size 1 (for simplicity)\n",
    "- because of the way the RNN state is passed from time step to time step, the model only accepts a fixed batch size once built\n",
    "- **to run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the last checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the file in the working directory that contains the most recent checkpoint\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a new RNN model instance\n",
    "rnn_cp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# load the saved weights from the checkpoint into the new model instance\n",
    "rnn_cp.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# build the model with a new input shape\n",
    "rnn_cp.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start by choosing a start string, initializing the RNN state and setting the number of characters to generate\n",
    "- get the prediction distribution of the next character using the start string and the RNN state\n",
    "- then, use a categorical distribution to calculate the index of the predicted character\n",
    "- use this predicted character as our next input to the model\n",
    "- the RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character\n",
    "- after predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prediction function\n",
    "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
    "    \n",
    "    # num of chars to generate\n",
    "    num_generate = num_generate\n",
    "    \n",
    "    # vectorizing the start string to numbers\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input=input_eval, axis=0) # returns a tensor with a length-1 axis inserted at index `axis`\n",
    "    \n",
    "    # empty string to store results\n",
    "    text_generated = list()\n",
    "    \n",
    "    # \"temperature\"\n",
    "    # low temperature results in more predictable text,\n",
    "    # high temperature results in more surprising text.\n",
    "    # feel free to experiment with this parameter\n",
    "    temperature = 1.0\n",
    "    \n",
    "    # the batch size was defined when we loaded model weights from training\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # use a categorical distribution to predict the character returned by the model\n",
    "        preidctions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return(start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# text generation!\n",
    "print(generate_text(rnn_cp, start_string=u'Anthony simps hard for Janet', num_generate=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
