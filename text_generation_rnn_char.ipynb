{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow text generation with an RNN tutorial](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow GPU memory growth must be limited to allow model to train (was having issues without doing this).  Code in below cell borrowed from the [TensorFlow documentation](https://www.tensorflow.org/guide/gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# limiting GPU memory growth\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # enabling memory growth\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPU')\n",
    "    except RuntimeError as e:\n",
    "        # memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text must all be in a single `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download file\n",
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# # open the file\n",
    "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# # data type?\n",
    "# print(type(text))\n",
    "\n",
    "# # number of characters\n",
    "# print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Length of text: 811074 characters\n"
     ]
    }
   ],
   "source": [
    "# open the file\n",
    "text = str(open('data/drake_lyrics.txt', 'r').read())\n",
    "\n",
    "# data type?\n",
    "print(type(text))\n",
    "\n",
    "# number of characters\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo what's goin on, this is Drake\n",
      "And I'ma let you know what you about to witness aight?\n",
      "This right here, is a Drake, and DJ Smallz collaboration\n",
      "So I'm from Canada, my mans from down South\n",
      "You understand the #1 DJ in the South to be exact\n",
      "You heard t\n"
     ]
    }
   ],
   "source": [
    "# peek into file\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "100 unique characters\n"
     ]
    }
   ],
   "source": [
    "# unique characters in file\n",
    "vocab = sorted(set(text))\n",
    "print(type(vocab))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is **character vectorization**.  Word vectorization would probably make more coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Yo what's goi\" ---- characters mapped to int ---- > [54 71  1 79 64 57 76  8 75  1 63 71 65]\n"
     ]
    }
   ],
   "source": [
    "# map unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# reverse the map - use this to specify an index to obtain a character\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# entire text document represented in the above character-to-indices mapping\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# sample\n",
    "print(f'\"{text[:13]}\" ---- characters mapped to int ---- > {text_as_int[:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Examples & Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model input**: sequence of characters\n",
    "\n",
    "**model output (prediction)**: the following character at each step (based on previous characters in the sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the text into **example sequences**.  Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "**For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.**\n",
    "\n",
    "So, break the text into chunks of `seq_length + 1`.  e.g. if `seq_length` is 4 and our text is \"Hello\", the input sequence would be \"Hell\" and the target sequence would be \"ello\".\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices` converts the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "# max sentence length (in number of characters) desired for single input\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1) # floored division\n",
    "\n",
    "# create training examples/targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# data type of train examples/targets\n",
    "print(type(char_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      "o\n",
      " \n",
      "w\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# preview training examples as characters (using the indices in char_dataset)\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()]) # .numpy() converts into numpy data format (in this case, a numpy integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `batch` method on `char_dataset` (type `tensorflow.python.data.ops.dataset_ops.TensorSliceDataset`) to convert the individual characters to sequences of the desired size (`seq_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> \n",
      "\n",
      "\"Yo what's goin on, this is Drake\\nAnd I'ma let you know what you about to witness aight?\\nThis right he\"\n",
      "\"re, is a Drake, and DJ Smallz collaboration\\nSo I'm from Canada, my mans from down South\\nYou understan\"\n",
      "\"d the #1 DJ in the South to be exact\\nYou heard that at the VMA's, you heard it wherever he goes\\nMy ma\"\n",
      "\"n Smallz is out there down South\\nSame time reppin for Toronto, Canada y'knahmean?\\nSo this right here \"\n",
      "'what you \\'bout what you listenin to right NOW\\nis the official, \"Southern Smoke: Special Edition\"\\nI ca'\n"
     ]
    }
   ],
   "source": [
    "# create sequence batches from the char_dataset\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "print(type(sequences), '\\n')\n",
    "\n",
    "# preview some sequences\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text using the `map` method on the batch object to apple a simple function to each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shifting (splitting) function\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # up to but not including the last character\n",
    "    target_text = chunk[1:] # everything except for the firs tcharacter\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "# apply the shifting to create input texts and target texts that comprise of our dataset\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Yo what's goin on, this is Drake\\nAnd I'ma let you know what you about to witness aight?\\nThis right h\"\n",
      "Target data: \"o what's goin on, this is Drake\\nAnd I'ma let you know what you about to witness aight?\\nThis right he\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the first few examples of input and target values\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, at time step 0, the model receives the index for F (from \"First\") and tries to predict the \"i\" (from \"First\") as the next character.  At the next time step, it does the same thing, but the RNN considers the previous time step context in addition to the current input character (it would consider both \"F\" and \"i\" in trying to predict \"r\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW CELL CAUSES GPU MEMORY SPIKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first few examples of prediction time steps\n",
    "# for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "#     print(f\"Step {i:4d}\")\n",
    "#     print(f\"  input: {input_idx} ({repr(idx2char[input_idx]):s})\")\n",
    "#     print(f\"  expected output: {target_idx} ({repr(idx2char[target_idx]):s})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training *Batches*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` was used to split the text into _sequences_.  But before feeding this data into the model, we must _shuffle_ the data and pack it into _batches_.  The first layer of the model will be a Keras `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "# (TensorFlow data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory.  Instead,\n",
    "# it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset_sb = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset_sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be passed into an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions\n",
    "- `tf.keras.layers.GRU`: A type of RNN with size `units = rnn_units` (You can also use an LSTM layer here)\n",
    "- `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary length (number of characters)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to quickly build the RNN model based on vocab size, embedding dimension, number of RNN units, and batch size\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = vocab_size,\n",
    "        output_dim = embedding_dim,\n",
    "        batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.GRU(\n",
    "        units = rnn_units,\n",
    "        return_sequences = True,\n",
    "        stateful = True,\n",
    "        recurrent_initializer = 'glorot_uniform'\n",
    "    ))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "rnn = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Model (Without Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 100) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset_sb.take(1):\n",
    "    example_batch_predictions = rnn(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence length (`seq_length`) was set to `100` but the model can be run on inputs of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           25600     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 100)           102500    \n",
      "=================================================================\n",
      "Total params: 4,066,404\n",
      "Trainable params: 4,066,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we must sample from the output distribution to get actual character indices.  This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "**Note**: It is important to _sample_ from this distribution, since taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # tf.squeeze() removes all size-1 dimensions from the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 19, 26, 34, 47, 70, 41, 85, 53, 80, 13, 77, 10, 71, 99, 55,  7,\n",
       "        2, 64, 35, 93, 61, 61,  9, 23, 79, 34, 95, 92, 63, 48, 17, 42, 11,\n",
       "       92,  2, 22, 40, 49, 27, 66, 48, 17, 41, 43, 48, 15, 64, 16,  5, 35,\n",
       "       92, 75, 36, 31, 92, 69, 45, 39,  0, 42, 50, 77, 27,  0, 48, 77, 90,\n",
       "       19, 96, 13, 85, 68, 16, 31, 90, 47, 29, 14, 99, 13, 66, 45, 68, 63,\n",
       "       96, 39, 68, 14, 27, 78, 27, 51,  6, 79, 18, 12,  0, 39, 99],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "display(sampled_indices)\n",
    "print(len(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by the untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"igher\\n\\nYeah I feel it in my sleep\\n\\nI said I'm gettin' higher\\n\\nYeah\\n\\nAnd closer to my dreams\\n\\nWhoa\\n\\nS\"\n",
      "\n",
      "Output: '?29ERnL©Xx,u)o…Z&!hF–ee(6wE‘úgS0M*ú!5KT:jS0LNS.h/$FúsGBúmPJ\\nMUu:\\nSuñ2’,©l/BñR?-…,jPlg’Jl-:v:V%w1+\\nJ…'\n"
     ]
    }
   ],
   "source": [
    "print(f'Input: {repr(\"\".join(idx2char[input_example_batch[0]]))}\\n')\n",
    "print(f'Output: {repr(\"\".join(idx2char[sampled_indices]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a classification problem: **Given the previous RNN state, and the input at this time step, predict the class of the next character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching an Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because the model returns logits, we need to set the `from_logits` flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to obtain the loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 100)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.606465\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method.  Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "\n",
    "# create checkpoints-saving object\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    monitor = 'loss',\n",
    "    save_best_only = True,\n",
    "    mode = 'min',\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of desired epochs\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.9170 - accuracy: 0.4450\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.6631 - accuracy: 0.5089\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.5148 - accuracy: 0.5487\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.4106 - accuracy: 0.5785\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.3289 - accuracy: 0.6019\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 1.2532 - accuracy: 0.6241\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 1.1791 - accuracy: 0.6467\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 1.1109 - accuracy: 0.6686\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 1.0414 - accuracy: 0.6905\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.9721 - accuracy: 0.7137\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.9063 - accuracy: 0.7352\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.8427 - accuracy: 0.7570\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.7832 - accuracy: 0.7770\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.7291 - accuracy: 0.7956\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.6820 - accuracy: 0.8113\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.6411 - accuracy: 0.8258\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.6052 - accuracy: 0.8378\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.5780 - accuracy: 0.8475\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.5525 - accuracy: 0.8559\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.5326 - accuracy: 0.8631\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.5139 - accuracy: 0.8694\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4983 - accuracy: 0.8746\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4858 - accuracy: 0.8792\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4759 - accuracy: 0.8825\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4667 - accuracy: 0.8858\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4566 - accuracy: 0.8891\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4514 - accuracy: 0.8914\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4452 - accuracy: 0.8932\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4412 - accuracy: 0.8947\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4348 - accuracy: 0.8961\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4316 - accuracy: 0.8978\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4268 - accuracy: 0.8996\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4228 - accuracy: 0.9006\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4195 - accuracy: 0.9018\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4177 - accuracy: 0.9023\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4159 - accuracy: 0.9024\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4145 - accuracy: 0.9028\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 11s 91ms/step - loss: 0.4126 - accuracy: 0.90362s - loss: 0.4076 - ac -\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 12s 92ms/step - loss: 0.4106 - accuracy: 0.9043\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 11s 91ms/step - loss: 0.4106 - accuracy: 0.9044\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4088 - accuracy: 0.9046\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4092 - accuracy: 0.9040\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4097 - accuracy: 0.9039\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4104 - accuracy: 0.9036\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 11s 90ms/step - loss: 0.4086 - accuracy: 0.9044\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.4048 - accuracy: 0.9055\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4067 - accuracy: 0.9045\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4064 - accuracy: 0.9048\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4057 - accuracy: 0.9049\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.4056 - accuracy: 0.9046\n",
      "Wall time: 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training!\n",
    "history = rnn.fit(\n",
    "    x = dataset_sb,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text (Making Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the Latest Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch size 1 (for simplicity)\n",
    "- because of the way the RNN state is passed from time step to time step, the model only accepts a fixed batch size once built\n",
    "- **to run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the last checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\checkpoint'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the file in the working directory that contains the most recent checkpoint\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a new RNN model instance\n",
    "rnn_cp = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# load the saved weights from the checkpoint into the new model instance\n",
    "rnn_cp.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# build the model with a new input shape\n",
    "rnn_cp.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            25600     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 100)            102500    \n",
      "=================================================================\n",
      "Total params: 4,066,404\n",
      "Trainable params: 4,066,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start by choosing a start string, initializing the RNN state and setting the number of characters to generate\n",
    "- get the prediction distribution of the next character using the start string and the RNN state\n",
    "- then, use a categorical distribution to calculate the index of the predicted character\n",
    "- use this predicted character as our next input to the model\n",
    "- the RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character\n",
    "- after predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prediction function\n",
    "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
    "    \n",
    "    # num of chars to generate\n",
    "    num_generate = num_generate\n",
    "    \n",
    "    # vectorizing the start string to numbers\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input=input_eval, axis=0) # returns a tensor with a length-1 axis inserted at index `axis`\n",
    "    \n",
    "    # empty string to store results\n",
    "    text_generated = list()\n",
    "    \n",
    "    # \"temperature\"\n",
    "    # low temperature results in more predictable text,\n",
    "    # high temperature results in more surprising text.\n",
    "    # feel free to experiment with this parameter\n",
    "    temperature = 1.0\n",
    "    \n",
    "    # the batch size was defined when we loaded model weights from training\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # use a categorical distribution to predict the character returned by the model\n",
    "        preidctions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return(start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony simps hard for Janeth Lord\n",
      "Young but I'm a spend time\n",
      "(Will me got big money can buy when you die, you working with me\n",
      "You don't care, it's gon' be actin degrets\n",
      "\n",
      "For the Fact\n",
      "These are used to probably well that Now, it’signated?\n",
      "\n",
      "I followed you my in the sin\n",
      "You'de do a purchase, get it, uh, uh, uh heard things\n",
      "Niggas want my spot and don't ading meane\n",
      "Now it's bunna to leave\n",
      "Know that I'm gonna have to see you for 1M Soforethin\n",
      "Just remember who I was on their ass is what Ken a M2 From my feelings\n",
      "Overse be a thucked out that online race, oh oh\n",
      "The 9ation here we go a head\n",
      "Throw your ones up\n",
      "I can't believe you,\n",
      "\n",
      "Uh, oh hi, work the nigga?\n",
      "Demax, feel alice fan you know a nigga promise you come arove\n",
      "Me, I'm a break down puttin' on like... before it all hurt, another nigga we made it!\n",
      "You don't know my country's ever seem losin' in school\n",
      "You know I gotta hide it and cutfish you,\n",
      "If he don't mean to, always loosed up\n",
      "And when I got shut to make you high up\n",
      "I give you my all\n",
      "I get the girl that I ever w\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# text generation!\n",
    "print(generate_text(rnn_cp, start_string=u'Anthony simps hard for Janet', num_generate=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODIFY TRAINING TO BE CUSTOMIZABLE (per tutorial)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFY RNN ARCHITECTURE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
